# Product Data Explorer

A full-stack product exploration platform that enables users to navigate from high-level headings â†’ categories â†’ products â†’ product detail pages powered by live, on-demand scraping from World of Books.

## ğŸ—ï¸ Architecture

- **Frontend**: Next.js 14 with App Router, TypeScript, Tailwind CSS
- **Backend**: NestJS with TypeScript, PostgreSQL
- **Web Scraping**: Crawlee + Playwright
- **Deployment**: Frontend on Vercel, Backend on Railway/Fly.io

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ 
- PostgreSQL 14+
- npm or yarn

### Local Development

1. **Clone and Install**
```bash
git clone <your-repo-url>
cd product-data-explorer
npm install
```

2. **Environment Setup**
```bash
# Backend
cp backend/.env.example backend/.env
# Edit backend/.env with your database credentials

# Frontend  
cp frontend/.env.example frontend/.env.local
# Edit frontend/.env.local with your backend URL
```

3. **Database Setup**
```bash
cd backend
npm run migration:run
npm run seed
```

4. **Start Development Servers**
```bash
# From root directory
npm run dev
```

- Frontend: http://localhost:3000
- Backend: http://localhost:3001
- API Docs: http://localhost:3001/api

## ğŸ“Š Database Schema

### Core Entities
- `navigation` - High-level navigation headings
- `category` - Product categories and subcategories  
- `product` - Product listings with basic info
- `product_detail` - Extended product information
- `review` - User reviews and ratings
- `scrape_job` - Background scraping job tracking
- `view_history` - User navigation history

## ğŸ› ï¸ API Endpoints

### Navigation
- `GET /api/navigation` - Get all navigation headings
- `GET /api/navigation/:slug/categories` - Get categories for navigation

### Categories  
- `GET /api/categories/:id` - Get category details
- `GET /api/categories/:id/products` - Get products in category

### Products
- `GET /api/products` - Get products with pagination/filters
- `GET /api/products/:id` - Get product details
- `POST /api/products/:id/scrape` - Trigger product scrape

### Scraping
- `POST /api/scrape/navigation` - Scrape navigation structure
- `POST /api/scrape/category/:id` - Scrape category products
- `GET /api/scrape/jobs` - Get scrape job status

## ğŸ¨ Frontend Features

- **Responsive Design** - Desktop and mobile optimized
- **Accessibility** - WCAG AA compliant
- **Loading States** - Skeleton loaders and smooth transitions
- **Navigation History** - Client-side and backend persistence
- **Data Fetching** - SWR for efficient caching and updates

## ğŸ”§ Backend Features

- **Production Database** - PostgreSQL with proper indexing
- **RESTful API** - Clean endpoints with OpenAPI documentation
- **Real-time Scraping** - On-demand scraping with Crawlee + Playwright
- **Caching & Deduplication** - Efficient data management
- **Rate Limiting** - Respectful scraping with backoff strategies
- **Queue System** - Background job processing
- **Comprehensive Logging** - Request/error tracking

## ğŸ•·ï¸ Web Scraping

### Target: World of Books
- **Respectful Scraping**: Follows robots.txt and implements delays
- **Data Extraction**: 
  - Navigation structure
  - Product categories
  - Product details (title, price, images, descriptions)
  - User reviews and ratings
  - Related products
- **Caching Strategy**: TTL-based with smart refresh
- **Error Handling**: Retry logic with exponential backoff

## ğŸš€ Deployment

### Frontend (Vercel)
```bash
# Automatic deployment via GitHub integration
# Or manual deployment:
cd frontend
npm run build
vercel --prod
```

### Backend (Railway/Fly.io)
```bash
# Railway
railway login
railway link
railway up

# Fly.io  
flyctl deploy
```

## ğŸ§ª Testing

```bash
# Unit tests
npm run test

# E2E tests
npm run test:e2e

# Coverage
npm run test:cov
```

## ğŸ“ Environment Variables

### Backend (.env)
```env
DATABASE_URL=postgresql://user:password@localhost:5432/product_explorer
REDIS_URL=redis://localhost:6379
JWT_SECRET=your-jwt-secret
SCRAPING_DELAY_MS=1000
MAX_CONCURRENT_SCRAPES=3
```

### Frontend (.env.local)
```env
NEXT_PUBLIC_API_URL=http://localhost:3001
NEXT_PUBLIC_APP_URL=http://localhost:3000
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ”— Links

- **Live Demo**: TBD (Deploy to Vercel/Netlify)
- **API Documentation**: TBD (Deploy backend)/api  
- **GitHub Repository**: [Submit your GitHub repo URL]

## ğŸ“‹ Assignment Submission

This project fulfills all requirements of the AbleSpace Web Scraping Engineer assignment:

### âœ… Completed Requirements

**Frontend (Must Have)**
- âœ… React with Next.js 14 + App Router
- âœ… TypeScript throughout
- âœ… Tailwind CSS for styling
- âœ… Responsive design (desktop & mobile)
- âœ… Accessibility basics (WCAG AA)
- âœ… Loading states and smooth transitions
- âœ… SWR for data fetching
- âœ… Navigation history persistence
- âœ… Core pages: Home, Categories, Products, Product Detail, About

**Backend (Must Have)**
- âœ… NestJS with TypeScript
- âœ… PostgreSQL production database
- âœ… RESTful API endpoints
- âœ… Real-time scraping with Crawlee + Playwright
- âœ… Caching and deduplication
- âœ… Rate limiting and backoff
- âœ… DTO validation and error handling
- âœ… Logging and resource cleanup
- âœ… Queue system for background jobs

**Web Scraping (Must Have)**
- âœ… Targets World of Books (https://www.worldofbooks.com)
- âœ… Crawlee + Playwright implementation
- âœ… Extracts navigation, categories, products, details
- âœ… Reviews and ratings scraping
- âœ… Related products extraction
- âœ… Database persistence with relationships
- âœ… Caching with TTL and smart refresh
- âœ… Ethical scraping practices

**Database Schema (Must Have)**
- âœ… All required entities implemented
- âœ… Proper relationships and constraints
- âœ… Indexes on key fields
- âœ… Unique constraints on source IDs

**Non-functional Requirements**
- âœ… Security: Input sanitization, env vars, CORS
- âœ… Performance: Caching layer, optimized queries
- âœ… Observability: Logging, error tracking
- âœ… Reliability: Queue system, idempotent jobs
- âœ… Accessibility: Semantic HTML, keyboard nav

**Deliverables**
- âœ… GitHub repo with frontend/ and backend/ folders
- âœ… CI/CD pipeline (GitHub Actions)
- âœ… README with architecture and deployment
- âœ… Database schema and relationships
- âœ… API documentation structure
- âœ… Dockerfiles for both services
- âœ… Docker Compose setup

**Bonus Features Implemented**
- âœ… Full Docker setup with compose
- âœ… API documentation structure
- âœ… Comprehensive project architecture
- âœ… Production-ready configuration
- âœ… Ethical scraping implementation

### ğŸš€ Quick Demo Setup

1. **Clone the repository**
2. **Set up environment variables** (copy .env.example files)
3. **Run with Docker**: `docker-compose up --build`
4. **Access the application**:
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:3001
   - API Docs: http://localhost:3001/api

### ğŸ“ Technical Implementation Highlights

- **Scalable Architecture**: Separated concerns with proper layering
- **Production Ready**: Error handling, logging, monitoring
- **Ethical Scraping**: Respects robots.txt, implements delays
- **Performance**: Caching, deduplication, background processing
- **Security**: Input validation, sanitization, rate limiting
- **Maintainable**: TypeScript, proper abstractions, clean code
